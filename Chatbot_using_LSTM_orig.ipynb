{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot_using_LSTM_orig.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachhh53/chatbot/blob/main/Chatbot_using_LSTM_orig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xoKYBBO6xaV"
      },
      "source": [
        "# **Chatbot using Seq2Seq LSTM models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SOmE99B7Is0"
      },
      "source": [
        "This project is to create conversational chatbot using Sequence to sequence LSTM models. \n",
        "Sequence to sequence learning is about training models to convert from one domain to sequences another domain. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVuZTAV08qWY"
      },
      "source": [
        "# Step 1: Import all the packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0mJXRse83hp"
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL0Rz6JZ9eLW"
      },
      "source": [
        "# Step 2: Download all the data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79eY9jkA9lnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05fa5fb-59f5-49f9-dbd6-9b02ae2e894e"
      },
      "source": [
        "!pip install kaggle "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cU-sP689zyx",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "1872cc29-96cc-4002-8de9-58e1c66dbe14"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3f4b143d-a7d8-416e-9d90-72c6a773e9fb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3f4b143d-a7d8-416e-9d90-72c6a773e9fb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"rachiebess\",\"key\":\"3c496c8f56467a6050777e65bcdb80c2\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raW7lVdQ9z7k"
      },
      "source": [
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adp0zjFP90Bl"
      },
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPpi-Suj9z4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c37b20-28bb-4b5f-a878-ff69d7914593"
      },
      "source": [
        "!ls ~/.kaggle"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gocz2gG9zsC"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq0zmSYy-S0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff69b7f-9a9d-4c20-b2bb-ab4a30439769"
      },
      "source": [
        "!kaggle datasets download -d kausr25/chatterbotenglish"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading chatterbotenglish.zip to /content\n",
            "\r  0% 0.00/23.2k [00:00<?, ?B/s]\n",
            "\r100% 23.2k/23.2k [00:00<00:00, 19.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yk6kTtM-bQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28fb1c0-af38-4ac6-e4c7-5d0a77671ac6"
      },
      "source": [
        "!unzip /content/chatterbotenglish.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/chatterbotenglish.zip\n",
            "  inflating: ai.yml                  \n",
            "  inflating: botprofile.yml          \n",
            "  inflating: computers.yml           \n",
            "  inflating: emotion.yml             \n",
            "  inflating: food.yml                \n",
            "  inflating: gossip.yml              \n",
            "  inflating: greetings.yml           \n",
            "  inflating: health.yml              \n",
            "  inflating: history.yml             \n",
            "  inflating: humor.yml               \n",
            "  inflating: literature.yml          \n",
            "  inflating: money.yml               \n",
            "  inflating: movies.yml              \n",
            "  inflating: politics.yml            \n",
            "  inflating: psychology.yml          \n",
            "  inflating: science.yml             \n",
            "  inflating: sports.yml              \n",
            "  inflating: trivia.yml              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PMKhVnUHwGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5b931f-8531-4e24-b6e8-b5b21e7b5103"
      },
      "source": [
        "!wget https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true -O chatbot_nlp.zip\n",
        "!unzip chatbot_nlp.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-26 17:25:48--  https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/shubham0204/Dataset_Archives/raw/master/chatbot_nlp.zip [following]\n",
            "--2022-08-26 17:25:48--  https://github.com/shubham0204/Dataset_Archives/raw/master/chatbot_nlp.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/shubham0204/Dataset_Archives/master/chatbot_nlp.zip [following]\n",
            "--2022-08-26 17:25:48--  https://raw.githubusercontent.com/shubham0204/Dataset_Archives/master/chatbot_nlp.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24292 (24K) [application/zip]\n",
            "Saving to: ‘chatbot_nlp.zip’\n",
            "\n",
            "chatbot_nlp.zip     100%[===================>]  23.72K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-08-26 17:25:49 (8.08 MB/s) - ‘chatbot_nlp.zip’ saved [24292/24292]\n",
            "\n",
            "Archive:  chatbot_nlp.zip\n",
            "   creating: chatbot_nlp/\n",
            "   creating: chatbot_nlp/data/\n",
            "  inflating: chatbot_nlp/data/ai.yml  \n",
            "  inflating: chatbot_nlp/data/botprofile.yml  \n",
            "  inflating: chatbot_nlp/data/computers.yml  \n",
            "  inflating: chatbot_nlp/data/emotion.yml  \n",
            "  inflating: chatbot_nlp/data/food.yml  \n",
            "  inflating: chatbot_nlp/data/gossip.yml  \n",
            "  inflating: chatbot_nlp/data/greetings.yml  \n",
            "  inflating: chatbot_nlp/data/health.yml  \n",
            "  inflating: chatbot_nlp/data/history.yml  \n",
            "  inflating: chatbot_nlp/data/humor.yml  \n",
            "  inflating: chatbot_nlp/data/literature.yml  \n",
            "  inflating: chatbot_nlp/data/money.yml  \n",
            "  inflating: chatbot_nlp/data/movies.yml  \n",
            "  inflating: chatbot_nlp/data/politics.yml  \n",
            "  inflating: chatbot_nlp/data/psychology.yml  \n",
            "  inflating: chatbot_nlp/data/science.yml  \n",
            "  inflating: chatbot_nlp/data/sports.yml  \n",
            "  inflating: chatbot_nlp/data/trivia.yml  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4kJp6uO-fQE"
      },
      "source": [
        "# Step 3: Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEV_hSXs-7mF"
      },
      "source": [
        "### a) Reading the data from the files\n",
        "We parse each of the .yaml files.\n",
        "\n",
        "1. Concatenate two or more sentences if the answer has two or more of them.\n",
        "2. Remove unwanted data types which are produced while parsing the data.\n",
        "3. Append <START> and <END> to all the answers.\n",
        "4. Create a Tokenizer and load the whole vocabulary ( questions + answers ) into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWYOjzOc_iQi"
      },
      "source": [
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyUnopqjDjud"
      },
      "source": [
        "The dataset contains .yml files which have pairs of different questions and their answers on varied subjects like history, bot profile, science etc.\n",
        "We can easily read them as folows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxG-s4k0CowI"
      },
      "source": [
        "dir_path = '/content/chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path + os.sep)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bRvbQ00Coy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8705a6c3-8f9f-4624-fa58-73a89e791852"
      },
      "source": [
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE : 1894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMPqb8LxIeGI"
      },
      "source": [
        "### b) Preparing data for Seq2Seq model\n",
        "\n",
        "This model requires 3 arrays encoder_input_data, decoder_input_data and decoder_output_data.\n",
        "\n",
        "For encoder_input_data:\n",
        "Tokensize the Questions and Pad them to their maximum Length.\n",
        "\n",
        "For decoder_input_data:\n",
        "Tokensize the Answers and Pad them to their maximum Length.\n",
        "\n",
        "For decoder_output_data:\n",
        "Tokensize the Answers and Remove the 1st element from all the tokenized_answers. This is the <START> element which was added earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEfAPL4HCo1t"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqYoDsbSCo4f"
      },
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "  vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "  tokens_list = []\n",
        "  vocabulary = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    tokens = sentence.split()\n",
        "    vocabulary += tokens\n",
        "    tokens_list.append(tokens)\n",
        "  return tokens_list, vocabulary"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vKhieIwCo7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5893ca7d-6094-4d51-f29b-f2edac94bc45"
      },
      "source": [
        "#encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max( [len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "print(encoder_input_data.shape, maxlen_questions)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 22) 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJo7WPjLCo-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9a8400-ba47-4393-c226-543d00bdbd66"
      },
      "source": [
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 74) 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccY0wWdRCpCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f551554-39e5-48aa-9494-6129c105fce0"
      },
      "source": [
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 74, 1894)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D53pyucPCnk"
      },
      "source": [
        "# Step 4: Defining Encoder Decoder Model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3YjCFDwPRVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee88676-a9c6-4416-be47-d616616b9837"
      },
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 22)]         0           []                               \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 74)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 22, 200)      378800      ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 74, 200)      378800      ['input_10[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 200),        320800      ['embedding_2[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 74, 200),    320800      ['embedding_3[0][0]',            \n",
            "                                 (None, 200),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 74, 1894)     380694      ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,779,894\n",
            "Trainable params: 1,779,894\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVfSormAPb3w"
      },
      "source": [
        "# Step 5: Training the Model\n",
        "\n",
        "We train the model for a number of epochs with RMSprop optimizer and categorical_crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHlqQq64PYTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a38055-1f29-4258-eae0-63640f597e77"
      },
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=5, epochs=150 ) \n",
        "model.save( 'model.h7' )"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "113/113 [==============================] - 35s 219ms/step - loss: 1.1977\n",
            "Epoch 2/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 1.0416\n",
            "Epoch 3/150\n",
            "113/113 [==============================] - 24s 213ms/step - loss: 0.9991\n",
            "Epoch 4/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.9541\n",
            "Epoch 5/150\n",
            "113/113 [==============================] - 26s 231ms/step - loss: 0.9100\n",
            "Epoch 6/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.8701\n",
            "Epoch 7/150\n",
            "113/113 [==============================] - 24s 213ms/step - loss: 0.8326\n",
            "Epoch 8/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.7967\n",
            "Epoch 9/150\n",
            "113/113 [==============================] - 26s 229ms/step - loss: 0.7627\n",
            "Epoch 10/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.7302\n",
            "Epoch 11/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.6980\n",
            "Epoch 12/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.6659\n",
            "Epoch 13/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.6342\n",
            "Epoch 14/150\n",
            "113/113 [==============================] - 26s 226ms/step - loss: 0.6036\n",
            "Epoch 15/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.5711\n",
            "Epoch 16/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.5401\n",
            "Epoch 17/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.5089\n",
            "Epoch 18/150\n",
            "113/113 [==============================] - 26s 229ms/step - loss: 0.4779\n",
            "Epoch 19/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.4473\n",
            "Epoch 20/150\n",
            "113/113 [==============================] - 25s 219ms/step - loss: 0.4190\n",
            "Epoch 21/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.3910\n",
            "Epoch 22/150\n",
            "113/113 [==============================] - 25s 225ms/step - loss: 0.3647\n",
            "Epoch 23/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.3382\n",
            "Epoch 24/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.3150\n",
            "Epoch 25/150\n",
            "113/113 [==============================] - 25s 220ms/step - loss: 0.2915\n",
            "Epoch 26/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.2721\n",
            "Epoch 27/150\n",
            "113/113 [==============================] - 26s 228ms/step - loss: 0.2515\n",
            "Epoch 28/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.2334\n",
            "Epoch 29/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.2172\n",
            "Epoch 30/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.2017\n",
            "Epoch 31/150\n",
            "113/113 [==============================] - 26s 228ms/step - loss: 0.1868\n",
            "Epoch 32/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.1748\n",
            "Epoch 33/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.1612\n",
            "Epoch 34/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.1491\n",
            "Epoch 35/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.1388\n",
            "Epoch 36/150\n",
            "113/113 [==============================] - 26s 227ms/step - loss: 0.1294\n",
            "Epoch 37/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.1199\n",
            "Epoch 38/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.1112\n",
            "Epoch 39/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.1044\n",
            "Epoch 40/150\n",
            "113/113 [==============================] - 26s 229ms/step - loss: 0.0981\n",
            "Epoch 41/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0904\n",
            "Epoch 42/150\n",
            "113/113 [==============================] - 24s 214ms/step - loss: 0.0839\n",
            "Epoch 43/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0783\n",
            "Epoch 44/150\n",
            "113/113 [==============================] - 26s 227ms/step - loss: 0.0740\n",
            "Epoch 45/150\n",
            "113/113 [==============================] - 24s 217ms/step - loss: 0.0685\n",
            "Epoch 46/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0639\n",
            "Epoch 47/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.0595\n",
            "Epoch 48/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.0558\n",
            "Epoch 49/150\n",
            "113/113 [==============================] - 25s 224ms/step - loss: 0.0529\n",
            "Epoch 50/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0503\n",
            "Epoch 51/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0469\n",
            "Epoch 52/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0438\n",
            "Epoch 53/150\n",
            "113/113 [==============================] - 26s 227ms/step - loss: 0.0414\n",
            "Epoch 54/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0397\n",
            "Epoch 55/150\n",
            "113/113 [==============================] - 24s 217ms/step - loss: 0.0373\n",
            "Epoch 56/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.0352\n",
            "Epoch 57/150\n",
            "113/113 [==============================] - 26s 226ms/step - loss: 0.0331\n",
            "Epoch 58/150\n",
            "113/113 [==============================] - 24s 217ms/step - loss: 0.0314\n",
            "Epoch 59/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0300\n",
            "Epoch 60/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0291\n",
            "Epoch 61/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0297\n",
            "Epoch 62/150\n",
            "113/113 [==============================] - 26s 227ms/step - loss: 0.0276\n",
            "Epoch 63/150\n",
            "113/113 [==============================] - 25s 219ms/step - loss: 0.0261\n",
            "Epoch 64/150\n",
            "113/113 [==============================] - 25s 220ms/step - loss: 0.0249\n",
            "Epoch 65/150\n",
            "113/113 [==============================] - 25s 222ms/step - loss: 0.0237\n",
            "Epoch 66/150\n",
            "113/113 [==============================] - 26s 232ms/step - loss: 0.0229\n",
            "Epoch 67/150\n",
            "113/113 [==============================] - 25s 220ms/step - loss: 0.0220\n",
            "Epoch 68/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.0215\n",
            "Epoch 69/150\n",
            "113/113 [==============================] - 25s 219ms/step - loss: 0.0210\n",
            "Epoch 70/150\n",
            "113/113 [==============================] - 26s 227ms/step - loss: 0.0204\n",
            "Epoch 71/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.0199\n",
            "Epoch 72/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0196\n",
            "Epoch 73/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.0193\n",
            "Epoch 74/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.0187\n",
            "Epoch 75/150\n",
            "113/113 [==============================] - 25s 224ms/step - loss: 0.0182\n",
            "Epoch 76/150\n",
            "113/113 [==============================] - 24s 214ms/step - loss: 0.0181\n",
            "Epoch 77/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0179\n",
            "Epoch 78/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.0190\n",
            "Epoch 79/150\n",
            "113/113 [==============================] - 26s 228ms/step - loss: 0.0204\n",
            "Epoch 80/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.0212\n",
            "Epoch 81/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.0205\n",
            "Epoch 82/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.0180\n",
            "Epoch 83/150\n",
            "113/113 [==============================] - 26s 230ms/step - loss: 0.0163\n",
            "Epoch 84/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0156\n",
            "Epoch 85/150\n",
            "113/113 [==============================] - 25s 218ms/step - loss: 0.0150\n",
            "Epoch 86/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.0148\n",
            "Epoch 87/150\n",
            "113/113 [==============================] - 26s 228ms/step - loss: 0.0147\n",
            "Epoch 88/150\n",
            "113/113 [==============================] - 25s 217ms/step - loss: 0.0146\n",
            "Epoch 89/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0143\n",
            "Epoch 90/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0142\n",
            "Epoch 91/150\n",
            "113/113 [==============================] - 25s 225ms/step - loss: 0.0142\n",
            "Epoch 92/150\n",
            "113/113 [==============================] - 24s 216ms/step - loss: 0.0139\n",
            "Epoch 93/150\n",
            "113/113 [==============================] - 24s 214ms/step - loss: 0.0138\n",
            "Epoch 94/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0138\n",
            "Epoch 95/150\n",
            "113/113 [==============================] - 26s 227ms/step - loss: 0.0141\n",
            "Epoch 96/150\n",
            "113/113 [==============================] - 25s 220ms/step - loss: 0.0137\n",
            "Epoch 97/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0141\n",
            "Epoch 98/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0138\n",
            "Epoch 99/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0135\n",
            "Epoch 100/150\n",
            "113/113 [==============================] - 25s 225ms/step - loss: 0.0153\n",
            "Epoch 101/150\n",
            "113/113 [==============================] - 24s 214ms/step - loss: 0.0173\n",
            "Epoch 102/150\n",
            "113/113 [==============================] - 24s 214ms/step - loss: 0.0183\n",
            "Epoch 103/150\n",
            "113/113 [==============================] - 24s 212ms/step - loss: 0.0173\n",
            "Epoch 104/150\n",
            "113/113 [==============================] - 25s 224ms/step - loss: 0.0143\n",
            "Epoch 105/150\n",
            "113/113 [==============================] - 24s 213ms/step - loss: 0.0135\n",
            "Epoch 106/150\n",
            "113/113 [==============================] - 24s 215ms/step - loss: 0.0129\n",
            "Epoch 107/150\n",
            "113/113 [==============================] - 24s 213ms/step - loss: 0.0128\n",
            "Epoch 108/150\n",
            "113/113 [==============================] - 25s 225ms/step - loss: 0.0128\n",
            "Epoch 109/150\n",
            "113/113 [==============================] - 24s 212ms/step - loss: 0.0126\n",
            "Epoch 110/150\n",
            "113/113 [==============================] - 24s 213ms/step - loss: 0.0126\n",
            "Epoch 111/150\n",
            "113/113 [==============================] - 24s 214ms/step - loss: 0.0124\n",
            "Epoch 112/150\n",
            "113/113 [==============================] - 25s 223ms/step - loss: 0.0122\n",
            "Epoch 113/150\n",
            "113/113 [==============================] - 24s 213ms/step - loss: 0.0124\n",
            "Epoch 114/150\n",
            "113/113 [==============================] - 24s 212ms/step - loss: 0.0123\n",
            "Epoch 115/150\n",
            "113/113 [==============================] - 24s 214ms/step - loss: 0.0123\n",
            "Epoch 116/150\n",
            "113/113 [==============================] - 25s 222ms/step - loss: 0.0122\n",
            "Epoch 117/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0122\n",
            "Epoch 118/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0121\n",
            "Epoch 119/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0122\n",
            "Epoch 120/150\n",
            "113/113 [==============================] - 25s 221ms/step - loss: 0.0124\n",
            "Epoch 121/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0125\n",
            "Epoch 122/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0121\n",
            "Epoch 123/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0123\n",
            "Epoch 124/150\n",
            "113/113 [==============================] - 25s 219ms/step - loss: 0.0120\n",
            "Epoch 125/150\n",
            "113/113 [==============================] - 24s 209ms/step - loss: 0.0120\n",
            "Epoch 126/150\n",
            "113/113 [==============================] - 24s 209ms/step - loss: 0.0118\n",
            "Epoch 127/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0119\n",
            "Epoch 128/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0118\n",
            "Epoch 129/150\n",
            "113/113 [==============================] - 25s 221ms/step - loss: 0.0118\n",
            "Epoch 130/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0118\n",
            "Epoch 131/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0117\n",
            "Epoch 132/150\n",
            "113/113 [==============================] - 24s 209ms/step - loss: 0.0116\n",
            "Epoch 133/150\n",
            "113/113 [==============================] - 25s 222ms/step - loss: 0.0116\n",
            "Epoch 134/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0117\n",
            "Epoch 135/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0118\n",
            "Epoch 136/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0120\n",
            "Epoch 137/150\n",
            "113/113 [==============================] - 25s 220ms/step - loss: 0.0202\n",
            "Epoch 138/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0231\n",
            "Epoch 139/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0176\n",
            "Epoch 140/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0131\n",
            "Epoch 141/150\n",
            "113/113 [==============================] - 25s 221ms/step - loss: 0.0122\n",
            "Epoch 142/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0118\n",
            "Epoch 143/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0116\n",
            "Epoch 144/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0116\n",
            "Epoch 145/150\n",
            "113/113 [==============================] - 25s 221ms/step - loss: 0.0115\n",
            "Epoch 146/150\n",
            "113/113 [==============================] - 24s 212ms/step - loss: 0.0114\n",
            "Epoch 147/150\n",
            "113/113 [==============================] - 24s 211ms/step - loss: 0.0115\n",
            "Epoch 148/150\n",
            "113/113 [==============================] - 24s 210ms/step - loss: 0.0114\n",
            "Epoch 149/150\n",
            "113/113 [==============================] - 25s 221ms/step - loss: 0.0114\n",
            "Epoch 150/150\n",
            "113/113 [==============================] - 24s 212ms/step - loss: 0.0115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f4fc7688a90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f4fc75f70d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1MIy1j9aVTo"
      },
      "source": [
        "# Step 6: Defining Inference Models\n",
        "\n",
        "Encoder Inference Model: Takes questions as input and outputs LSTM states (h and c)\n",
        "\n",
        "Decoder Inference Model: Takes in 2 inputs one are the LSTM states, second are the answer input sequences. it will o/p the answers for questions which fed to the encoder model and it's state values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpLowS27cn8X"
      },
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    \n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwoYVsBTeYra"
      },
      "source": [
        "# Step 7: Talking with the Chatbot\n",
        "\n",
        "define a method str_to_tokens which converts str questions to Integer tokens with padding.\n",
        "\n",
        "1. First, we take a question as input and predict the state values using enc_model.\n",
        "2. We set the state values in the decoder's LSTM.\n",
        "3. Then, we generate a sequence which contains the <start> element.\n",
        "4. We input this sequence in the dec_model.\n",
        "5. We replace the <start> element with the element which was predicted by the dec_model and update the state values.\n",
        "6. We carry out the above steps iteratively till we hit the <end> tag or the maximum answer length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA7Yx45Li3wo"
      },
      "source": [
        "# def str_to_tokens( sentence : str ):\n",
        "\n",
        "#     words = sentence.lower().split()\n",
        "#     tokens_list = list()\n",
        "  \n",
        "#     for word in words:\n",
        "#         tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "#     return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "  \n",
        "    for word in words:\n",
        "        # only add it to the token_list if it's in the vocabulary\n",
        "        if word in tokenizer.word_index.keys():\n",
        "          tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "\n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
      ],
      "metadata": {
        "id": "8EOKfGF0QzoC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUr4SQDveVb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce6bc9e-a8e4-43a8-d1ad-3a70ad75cad5"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter question : hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 74) for input KerasTensor(type_spec=TensorSpec(shape=(None, 74), dtype=tf.float32, name='input_10'), name='input_10', description=\"created by layer 'input_10'\"), but it was called on an input with incompatible shape (None, 1).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " greetings end\n",
            "Enter question : What is ai?\n",
            " ai is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind end\n",
            "Enter question : What was the civil war?\n",
            " it depends on the first 'real' computer it was developed at university of pennsylvania in 1946 you could say that the very first primitive computer was the jacquard loom which was a programmable loom that used punchcards to store the patterns it made this made it a reprogrammable mechanical device end\n",
            "Enter question : What is your name?\n",
            " i am still young by your standards end\n",
            "Enter question : Tell me a joke\n",
            " what do you get when you cross a cat and a killer end\n",
            "Enter question : Do you eat?\n",
            " i'm a computer i can't eat or drink end\n",
            "Enter question : What is history?\n",
            " history is the course of political economic and military events over time from the dawn of man to the age of ai ' end\n",
            "Enter question : What is a good movie?\n",
            " a computer is is an electronic device which takes information in digital form and performs a series of operations based on predetermined instructions to give some output the thing you're using to talk to me is a computer an electronic device capable of performing calculations at very high speed and with very high accuracy a device which maps one set of numbers onto another set of numbers end\n",
            "Enter question : How much money do you have?\n",
            " no need for material possessions end\n",
            "Enter question : goodbye\n",
            " greetings end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPNjNwxUl2DO"
      },
      "source": [
        "# Conversion to TFLite \n",
        "\n",
        "We can convert our seq2seq model to a TensorFlow Lite model so that we can use it on edge devices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywh_aJ-Ulxme"
      },
      "source": [
        "# !pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3C3SlT-mboI"
      },
      "source": [
        "# converter = tf.lite.TFLiteConverter.from_keras_model( enc_model )\n",
        "# buffer = converter.convert()\n",
        "# open( 'enc_model.tflite' , 'wb' ).write( buffer )\n",
        "\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model( dec_model )\n",
        "# open( 'dec_model.tflite' , 'wb' ).write( buffer )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}